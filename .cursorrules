# Cursor Rules for PepperPy Project

# Type Safety and Async Patterns
rules:
  - name: "Type Safety"
    patterns:
      - "from typing import Any, AsyncGenerator, Protocol, Sequence"
      - "async def"
      - "-> None"
      - "-> Any"
      - "-> AsyncGenerator[AIResponse, None]"
    message: "Ensure proper type hints and async patterns"

# Async/Await Patterns
async_patterns:
  - name: "Async Generator Handling"
    rules:
      - "Use async for with AsyncGenerator[T, None]"
      - "Don't await AsyncGenerator directly"
      - "Yield chunks in stream methods"
    examples:
      - "async for chunk in generator:"
      - "async def stream() -> AsyncGenerator[AIResponse, None]:"

  - name: "Async Method Patterns"
    rules:
      - "Use await only for async methods"
      - "Don't await synchronous methods (like logging)"
      - "Handle cleanup in finally blocks"
    examples:
      - "await client.initialize()"
      - "logger.info()  # No await for sync methods"

# Module Structure
module_structure:
  core:
    - "core/: Framework foundation and shared utilities"
    - "db/: All database operations (including vectors)"
    - "ai/: AI/ML operations"
    - "files/: File system operations"
    - "console/: Terminal interface"
    - "ui/: User interface components"

# Module Base Classes
base_classes:
  - name: "InitializableModule"
    methods:
      - "async def _initialize(self) -> None"
      - "async def _cleanup(self) -> None"
      - "def _ensure_initialized(self) -> None"
    properties:
      - "is_initialized: bool"

# Provider Pattern
providers:
  base_pattern:
    - "class {Name}Provider(AIProvider):"
    - "async def initialize(self) -> None"
    - "async def cleanup(self) -> None"
    - "async def complete(self, prompt: str, **kwargs: Any) -> AIResponse"
    - "async def stream(self, prompt: str, **kwargs: Any) -> AsyncGenerator[AIResponse, None]"
    - "async def get_embedding(self, text: str) -> list[float]"
    - "async def validate_response(self, response: Any) -> AIResponse"

# Team Pattern
teams:
  base_pattern:
    - "class {Name}Team(BaseTeam):"
    - "async def _initialize(self) -> None"
    - "async def _cleanup(self) -> None"
    - "async def execute_task(self, task: str, **kwargs: Any) -> AIResponse"

# Factory Pattern
factory_pattern:
  - "class {Name}Factory:"
  - "@staticmethod"
  - "def create_{item}(config: {Name}Config) -> {Name}"

# Configuration Pattern
configuration:
  base_pattern:
    - "class {Name}Config(BaseModel):"
    - "class Config:"
    - "    frozen = True"
  validation:
    - "from pydantic import BaseModel, Field"
    - "metadata: JsonDict = Field(default_factory=dict)"

# Validation Pattern
validation:
  base_pattern:
    - "class {Name}Validator(Validator[T, V])"
    - "async def validate(self, value: V) -> ValidationResult[T]"
    - "async def validate_many(self, values: Sequence[V]) -> Sequence[ValidationResult[T]]"

# Error Handling
error_handling:
  pattern: |
    try:
        self._ensure_initialized()
        # ... operation code ...
    except Exception as e:
        raise {Module}Error(f"Operation failed: {e}", cause=e)

# Documentation
documentation:
  - "\"\"\"Module docstring\"\"\""
  - "\"\"\"Class docstring\"\"\""
  - "\"\"\"Method docstring\"\"\""

# Testing Requirements
testing:
  - "100% test coverage"
  - "Async test patterns"
  - "Mock external services"
  - "Test all error cases"
  - "Test initialization/cleanup"

# Quality Requirements
quality:
  - "mypy --strict mode"
  - "ruff linting"
  - "black formatting (line length: 100)"
  - "pre-commit hooks"
  - "Protocol for interfaces"

# Module Composition
composition:
  - "Use dependency injection"
  - "Avoid circular dependencies"
  - "Clear module boundaries"
  - "Interface-based design"
  - "Proper initialization order"

# Resource Management
resources:
  - "Proper cleanup in _cleanup"
  - "Resource validation in _initialize"
  - "Connection pooling where applicable"
  - "Proper error handling for resources"

# LLM Interaction Patterns
llm_patterns:
  - name: "Prompt Engineering"
    rules:
      - "Use structured prompts with clear sections"
      - "Include role and context in prompts"
      - "Specify expected output format"
      - "Handle multi-turn conversations"
    examples:
      - |
        f"As {role} with expertise in {domain}, analyze:\n\n{content}\n\n"
        "Include:\n"
        "- Key points\n"
        "- Analysis\n"
        "- Recommendations"

  - name: "Error Handling"
    rules:
      - "Wrap LLM calls in try-except blocks"
      - "Provide specific error messages"
      - "Include original error cause"
      - "Handle rate limits and timeouts"
    pattern: |
      try:
          return await self._client.complete(prompt)
      except Exception as e:
          raise AIError(f"LLM operation failed: {e}", cause=e)

  - name: "Response Processing"
    rules:
      - "Validate response structure"
      - "Handle partial responses"
      - "Process streaming chunks"
      - "Implement retry logic"
    examples:
      - "async for chunk in response.stream():"
      - "yield AIResponse(content=chunk, metadata=metadata)"

# LLM Configuration
llm_configuration:
  base_pattern:
    - "class {Name}Config(BaseModel):"
    - "    model: str"
    - "    temperature: float = 0.7"
    - "    max_tokens: int | None = None"
    - "    stop_sequences: list[str] | None = None"
    - "    presence_penalty: float = 0.0"
    - "    frequency_penalty: float = 0.0"
    - "    metadata: dict[str, Any] = Field(default_factory=dict)"
  validation:
    - "from pydantic import BaseModel, Field, validator"
    - "@validator('temperature')"
    - "def validate_temperature(cls, v: float) -> float:"
    - "    if not 0 <= v <= 2:"
    - "        raise ValueError('Temperature must be between 0 and 2')"
    - "    return v"

# LLM Resource Management
llm_resources:
  - "Implement token counting"
  - "Handle rate limiting"
  - "Pool connections when possible"
  - "Cleanup context after completion"
  - "Monitor usage and costs"

# LLM Testing
llm_testing:
  - "Mock LLM responses"
  - "Test prompt templates"
  - "Validate response parsing"
  - "Test error scenarios"
  - "Check token limits"
  - "Verify streaming behavior"
